1)  Consider the following Web graph adjacency matrix:

A-->B
A-->C
B-->C
C-->D
D-->E
E-->B

Compute 5 iterations of the PageRank algorithm for this graph.  Assume that d=0.85

           A         B           C            D           E
START      0.2       0.2         0.2          0.2         0.2
1          0.03      0.285       0.285        0.2         0.2
2          0.03      0.213       0.285        0.272       0.2
3          0.03      0.213       0.224        0.272       0.261
4          0.03      0.265       0.224        0.220       0.261
5          0.03      0.265       0.224        0.220       0.217


2)  Explain why 0.85 might be a reasonable value for d.  What change in user behavior would account for a rise in d?  What about a decline?  What would you expect to happen for surfers on mobile phones?

d is roughly the percentage of the time that people click, rather than type something into the URL.  If d goes up, it might mean that
people are lazier about typing.  If d goes down, then it means people are newly enthusiastic about entering URLs.  Because text entry
on a phone is relatively difficult, we'd expect d to go up.


3)  Give three reasons that PageRank might be a misleading guide to page popularity.

Possible answers:
  -- Links are now generated by social and technical mechanisms, and are no longer a reliable "vote for quality"
  -- Spammers have learned to create link farms, again undermining the informative quality of the hyperlink.
  -- New client technologies like AJAX have rendered traditional surfing somewhat obsolete.  People don't click like they used to.
  -- Others possible, too.


4)  Consider these two small web pages:

A. "The quick brown fox is really cool"

B.  "The cool brown fox is really here."

What is the k-shingle Jaccardian Coefficient for these two documents when k=3?

The shingles for A are: "The quick brown", "quick brown fox", "brown fox is", "fox is really", and "is really cool".
The shingles for B are: "The cool brown", "cool brown fox", "brown fox is", "fox is really", and "is really here".

The intersection of these sets is:  "brown fox is", "fox is really".  (Size 2).
The union is: "The quick brown", "The cool brown", "quick brown fox", "cool brown fox", "brown fox is", "fox is really", "is really cool", "is really here".  (Size 8).

That means the Jaccardian coefficient is 2/8 == 0.25.

You would have to use a very lenient threshold in order to find these to be identical documents.


5)  Imagine that you have a distributed indexing system, and you have 10 machines at your disposal.  A machine may be assigned to parsing or to inverting, but not both.  You have N documents to process, each of which has an identical number of words.  Words are (surprisingly!) equally distributed over the alphabet.

a)  What is your keyspace segmentation strategy?

Just divide the alphabet in I equally-spaced regions, where I is the number of inverters.


b)  How many machines should you devote to parsing, and how many to inverting?

We want to maximize parallelism in the system by making sure each part of the job (parsing and inverting) has an equal amount of workload-per-CPU.

The workload for parsing is very roughly equal to the workload for generating an index of those terms.  In both cases, the code touches each doc/term pair in the corpus.  So, uh, 5, to each, right!  Right?


c)  OK, that's a bit of a trick question.  You can't completely know the answer just yet.  What other factor do you have to consider?

Right, there's also the collation step.  The inverters must collect all the files from remote machines, sort them, and process.  This puts an extra load on the inverter machines.  So, if there W words in the entire corpus:

  -- Parsing takes about O(W).
  -- Inversion takes about O(W).
  -- Sorting takes about O(WlogW).

  In the architecture we've described, Inversion and Sorting take place on the same bank of machines.  So we may want to devote extra machines to that bank to account for the intermediate sorting step.  In real life, there are several of these MapReduce-style workloads where sorting is the dominant cost.


d)  Now imagine that the distribution of words over the alphabet reflects English - that is, some regions of the alphabet are more heavily represented than others.  What does this change about the system?

We want to redesign the keyspace segmentation to reflect natural use of English.  Some keyspace regions (e.g., "y-z") will be relatively empty, so we don't want to devote an entire machine to them.  Keyspaces that contain unpopular parts of the alphabet should be relatively large.  Keyspaces that contain popular parts of the alphabet should be relatively small.  The goal is for an equal number of elements to fall into each keyspace segmentation.


6)  Imagine that you have a term-based segmentation scheme for your distributed search query processor.  Unfortunately for you, I just signed a deal with hot real-time messaging startup Dweebler, and I want to add the real-time river of "Dweebs" to the search index.  What problems will this pose for your system? 


The problem is that each new Dweeb will mean I have to update the index on the machines that correspond to every text term in the Dweeb.  With document-based segmentation, I could just add all the changes to a single box.




