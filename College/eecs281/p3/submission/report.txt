	For this project I chose to make a hash map of hash maps to hold all of the words that occur in all of the files.  First I indexed all of the filenames each of which had a corresponding hash map.  Then as I read through the file I indexed each of the words in the hash map that corresponded to that given filename.  I did that to allow constant time lookup when searching if a word appeared in a given file.  Then I created an index hash map which held each of the words.  As each word was indexed it had a corresponding list which held all of the filenames in which that word appeared.  That way when calculating relevance, I only had to search through the files where the word appeared.

	My search function could have been made faster by sorting all of the files by relevance as I was finding them.  In this project I simply found all of the relevances and made a list of them, and then executed a bubble sort on that list.  The bubble sort is a very slow algorithm, order O(n^2), and could be replaced by a faster one.  One method of doing this would be to create a sorted binary tree.  As each total relevance is calculated it could be inserted into the tree which is a logarithmic runtime.  Then when it came time to print the list of relevances a simple tree traversal is all that is needed.

	There does exist an easier way of calculating relevance.  If relevance was calculated by adding each of the relevances together as opposed to multiplying them.  Addition is much faster on a processing level than multiplication, so this would speed up the computation.  Also if addition was used much larger relevances could be calculated without running into overflows.
